# ================================================
# Clockify RAG Service Configuration
# ================================================
#
# ⚠️  OPTIONAL CONFIGURATION FILE
#
# This file is NOT required for basic operation!
# The system works out-of-box with sensible defaults.
# Only create a .env file if you need to override defaults.
#
# Current defaults (no .env needed):
# - LLM endpoint: http://10.127.0.192:11434 (⚠️  COMPANY VPN ONLY - change for your environment)
# - Chat model: qwen2.5:32b
# - Embed model: nomic-embed-text:latest
# - Provider: ollama
#
# To override: Copy this file to .env and uncomment/edit the values you want to change.
# ================================================

# ====== OLLAMA / LLM CONFIGURATION ======
# Ollama endpoint (ENVIRONMENT-SPECIFIC)
# ⚠️  Default is company VPN endpoint - change for your deployment!
#
# Common profiles:
#   - Local Ollama:    RAG_OLLAMA_URL=http://127.0.0.1:11434
#   - Company VPN:     RAG_OLLAMA_URL=http://10.127.0.192:11434
#   - Custom endpoint: RAG_OLLAMA_URL=http://your-host:port
#
# Uncomment and set for your environment:
# RAG_OLLAMA_URL=http://127.0.0.1:11434

# Generation model (default: qwen2.5:32b)
# RAG_CHAT_MODEL=qwen2.5:32b

# Embedding model (default: nomic-embed-text:latest)
# RAG_EMBED_MODEL=nomic-embed-text:latest

# Optional: set to "mock" to force the deterministic offline client (useful for CI)
# RAG_LLM_CLIENT=mock

# Legacy aliases (OLLAMA_URL / GEN_MODEL / EMB_MODEL) remain supported but new RAG_* names are preferred.

# ====== PROVIDER SELECTION (GPT-OSS-20B Integration) ======
# Select LLM provider: "ollama" (default) or "gpt-oss"
# - "ollama": Standard Ollama with qwen2.5:32b (32k context)
# - "gpt-oss": OpenAI's gpt-oss-20b reasoning model (128k context)
RAG_PROVIDER=ollama

# ====== GPT-OSS-20B CONFIGURATION ======
# These settings apply when RAG_PROVIDER=gpt-oss
# GPT-OSS-20B is OpenAI's open-weight 20B reasoning model with 128k context window

# Model name (default: gpt-oss:20b)
# RAG_GPT_OSS_MODEL=gpt-oss:20b

# Sampling parameters (OpenAI's recommended defaults: temp=1.0, top_p=1.0)
# For deterministic RAG QA, you may prefer lower temperature (e.g., 0.7)
# RAG_GPT_OSS_TEMPERATURE=1.0
# RAG_GPT_OSS_TOP_P=1.0

# Context window size (GPT-OSS supports up to 128k tokens)
# RAG_GPT_OSS_CTX_WINDOW=128000

# Context budget for RAG snippets (16k tokens = ~12.5% of 128k context)
# Allows packing 2-3x more retrieval context than qwen2.5:32b
# RAG_GPT_OSS_CTX_BUDGET=16000

# Chat timeout (180s to allow for reasoning traces, vs 120s for qwen)
# RAG_GPT_OSS_CHAT_TIMEOUT=180.0

# Additional sampling parameters for GPT-OSS (previously hardcoded)
# RAG_GPT_OSS_TOP_K=40
# RAG_GPT_OSS_REPEAT_PENALTY=1.05

# Example: Enable GPT-OSS-20B
# Uncomment these lines to use gpt-oss:20b instead of qwen2.5:32b
# RAG_PROVIDER=gpt-oss
# RAG_GPT_OSS_MODEL=gpt-oss:20b
# RAG_GPT_OSS_TEMPERATURE=0.7  # Lower for more deterministic QA
# RAG_GPT_OSS_CTX_BUDGET=16000

# ====== OLLAMA SAMPLING PARAMETERS ======
# These settings apply to standard Ollama models (qwen2.5:32b, etc.)
# Previously hardcoded, now configurable for easier tuning

# Temperature: Controls randomness (0.0=deterministic, 1.0=balanced, 2.0=creative)
# RAG_OLLAMA_TEMPERATURE=0.0

# Top P (nucleus sampling): Probability mass threshold (0.0-1.0)
# Lower values = more focused, higher values = more diverse
# RAG_OLLAMA_TOP_P=0.9

# Top K: Limits selection to top K tokens (1-200)
# Lower values = more focused, higher values = more diverse
# RAG_OLLAMA_TOP_K=40

# Repeat Penalty: Penalizes token repetition (1.0=none, 1.05-1.2=moderate, 1.5+=strong)
# RAG_OLLAMA_REPEAT_PENALTY=1.05

# Example: More creative Ollama responses
# RAG_OLLAMA_TEMPERATURE=0.7
# RAG_OLLAMA_TOP_P=0.95
# RAG_OLLAMA_TOP_K=50

# ====== AUTOMATIC LLM FALLBACK (Production Resilience) ======
# Enable automatic failover to a secondary model when primary is unavailable
# Fallback triggers on connection errors, timeouts, and 5xx server errors

# Enable/disable fallback (default: true - enabled by default)
# RAG_FALLBACK_ENABLED=true

# Fallback provider: "gpt-oss" (default) or "ollama"
# RAG_FALLBACK_PROVIDER=gpt-oss

# Fallback model to use (default: gpt-oss:20b)
# RAG_FALLBACK_MODEL=gpt-oss:20b

# Example configurations:
# 1. Default (Qwen → GPT-OSS fallback):
#    RAG_PROVIDER=ollama          # Primary: qwen2.5:32b
#    RAG_FALLBACK_ENABLED=true    # Fallback enabled
#    RAG_FALLBACK_PROVIDER=gpt-oss
#    RAG_FALLBACK_MODEL=gpt-oss:20b
#
# 2. Disable fallback (fail fast):
#    RAG_FALLBACK_ENABLED=false
#
# 3. Use Llama as fallback:
#    RAG_FALLBACK_PROVIDER=ollama
#    RAG_FALLBACK_MODEL=llama3:70b

# ====== EMBEDDING BACKEND ======
# Choose embedding backend: "local" (SentenceTransformer) or "ollama"
EMB_BACKEND=local

# ====== RETRIEVAL PARAMETERS ======
# Default top-K candidates for retrieval (1-100)
DEFAULT_TOP_K=15

# Number of snippets to pack in context (1-50)
DEFAULT_PACK_TOP=8

# Cosine similarity threshold (0.0-1.0)
DEFAULT_THRESHOLD=0.25

# Hybrid retrieval weight: alpha*BM25 + (1-alpha)*dense (0.0-1.0)
ALPHA=0.5

# Enable intent-based retrieval for improved accuracy
USE_INTENT_CLASSIFICATION=1

# ====== CONTEXT AND PERFORMANCE ======
# Context token budget (effective context is min of this and num_ctx*0.6)
CTX_BUDGET=12000

# LLM context window size
DEFAULT_NUM_CTX=32768

# Maximum tokens to generate
DEFAULT_NUM_PREDICT=512

# MMR lambda for diversity vs relevance (0.0-1.0)
MMR_LAMBDA=0.75

# ====== ANNOTATION NEAREST NEIGHBORS (ANN) ======
# ANN backend: "faiss" or "none" (for full-scan)
ANN=faiss

# FAISS parameters
ANN_NLIST=64
ANN_NPROBE=16
FAISS_CANDIDATE_MULTIPLIER=3
ANN_CANDIDATE_MIN=200

# ====== BM25 PARAMETERS (for technical documentation) ======
BM25_K1=1.2
BM25_B=0.65

# ====== TIMEOUT CONFIGURATION ======
# Embedding timeouts (connect/read in seconds)
EMB_CONNECT_TIMEOUT=3.0
EMB_READ_TIMEOUT=60.0

# Chat/LM timeouts (connect/read in seconds)
CHAT_CONNECT_TIMEOUT=3.0
CHAT_READ_TIMEOUT=120.0

# Reranking timeout (in seconds)
RERANK_READ_TIMEOUT=180.0

# ====== EMBEDDING BATCHING ======
# Parallel embedding generation
EMB_MAX_WORKERS=8
EMB_BATCH_SIZE=32

# ====== API GATEKEEPING ======
# API auth: set to "api_key" and provide comma-separated API_ALLOWED_KEYS to enforce shared secret auth
API_AUTH_MODE=none
API_ALLOWED_KEYS=
API_KEY_HEADER=x-api-key

# ====== CACHING AND RATE LIMITING ======
# Query cache size
CACHE_MAXSIZE=100

# Cache TTL in seconds
CACHE_TTL=3600

# Enable automatic cache persistence (disabled by default)
# When enabled: background thread saves cache every CACHE_AUTO_SAVE_INTERVAL seconds
# Prevents cache loss on crashes/restarts (especially useful for long-running services)
CACHE_AUTO_SAVE_ENABLED=0

# Auto-save interval in seconds (only applies if enabled)
# Lower values = more frequent saves (more disk I/O), higher values = less frequent saves (more potential data loss)
CACHE_AUTO_SAVE_INTERVAL=300  # 5 minutes

# Example: Enable auto-save for production
# CACHE_AUTO_SAVE_ENABLED=1
# CACHE_AUTO_SAVE_INTERVAL=180  # Save every 3 minutes

# Enable rate limiting (disabled by default for internal deployment)
# Set to true/1 for external/public APIs to prevent abuse
# When disabled: no rate limiting overhead (~5-10ms per request saved)
# When enabled: sliding window rate limiter with configurable limits
RATE_LIMIT_ENABLED=0

# Rate limiting: max requests per window (only applies if enabled)
RATE_LIMIT_REQUESTS=10

# Rate limiting window in seconds (only applies if enabled)
RATE_LIMIT_WINDOW=60

# Example: Enable rate limiting for public API
# RATE_LIMIT_ENABLED=1
# RATE_LIMIT_REQUESTS=100
# RATE_LIMIT_WINDOW=60  # 100 requests per minute

# ====== BUILD AND INDEXING ======
# Build lock TTL in seconds
BUILD_LOCK_TTL_SEC=900

# Warm-up on startup
WARMUP=1
# Auto-download required NLTK corpora on startup (set to 0 for fully offline images)
NLTK_AUTO_DOWNLOAD=1

# ====== CHUNKING PARAMETERS ======
# Chunk size and overlap
CHUNK_CHARS=1600
CHUNK_OVERLAP=200

# ====== QUERY LOGGING ======
# Query log file path
RAG_LOG_FILE=rag_queries.jsonl

# Include answer text in logs (set to 0 to redact for privacy)
RAG_LOG_INCLUDE_ANSWER=1

# Answer placeholder when redacted
RAG_LOG_ANSWER_PLACEHOLDER=[REDACTED]

# Include chunk text in logs (set to 0 for security/privacy)
RAG_LOG_INCLUDE_CHUNKS=0

# Enable strict citation validation
RAG_STRICT_CITATIONS=0

# ====== PROXY CONFIGURATION ======
# Enable proxy support (set to 1 to allow proxy usage)
ALLOW_PROXIES=0

# HTTP proxy URL (if needed)
HTTP_PROXY=
HTTPS_PROXY=

# ====== RETRIES ======
# Default number of retries for transient errors
DEFAULT_RETRIES=2

# ====== PRECOMPUTED FAQ CACHE ======
# Enable precomputed FAQ cache
FAQ_CACHE_ENABLED=0
FAQ_CACHE_PATH=faq_cache.json

# Maximum query length (for DoS protection)
MAX_QUERY_LENGTH=1000000

# Query expansion file path
CLOCKIFY_QUERY_EXPANSIONS=config/query_expansions.json

# Maximum query expansion file size (in bytes)
MAX_QUERY_EXPANSION_FILE_SIZE=10485760
